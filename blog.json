[
  {
    "title": "Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers — Pappone et al. (2025)",
    "source_url": "https://www.linkedin.com/posts/activity-7376190186503806977-ApkU?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAAzTDtoBh8KeVDRAqwRd0mUwfVpwfyirm80",
    "date": ""
  },
  {
    "title": "Statistical Methods in Generative AI — Dobriban (2025)",
    "source_url": "https://www.linkedin.com/posts/activity-7373279958103912448-n-Wq?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAzTDtoBh8KeVDRAqwRd0mUwfVpwfyirm80",
    "date": ""
  },
  {
    "title": "Massive Activations in Large Language Models — Sun et al. (2024)",
    "source_url": "https://www.linkedin.com/posts/activity-7369329515782627329-pgtv?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAzTDtoBh8KeVDRAqwRd0mUwfVpwfyirm80",
    "date": ""
  },
  {
    "title": "Group Sequence Policy Optimization — Zheng et al. (2025)",
    "source_url": "https://www.linkedin.com/posts/activity-7367123464568197120-MkgV?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAzTDtoBh8KeVDRAqwRd0mUwfVpwfyirm80",
    "date": ""
  },
  {
    "title": "Kimi-K2 Model Card",
    "source_url": "https://www.linkedin.com/posts/activity-7354453356410544128-ceQn?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAzTDtoBh8KeVDRAqwRd0mUwfVpwfyirm80",
    "date": ""
  },
  {
    "title": "Attention sinks from the graph perspective - Obsidian Publish",
    "source_url": "https://publish.obsidian.md/the-tensor-throne/Transformers+as+GNNs/Attention+sinks+from+the+graph+perspective",
    "date": ""
  }
]
